{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8156a045-db82-4fb1-99c0-d0c0bf379dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\å¥¶ç½—å’ª\\appdata\\roaming\\python\\python313\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: biopython in c:\\users\\å¥¶ç½—å’ª\\appdata\\roaming\\python\\python313\\site-packages (1.85)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy scikit-learn biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135b3438-d4c2-48d4-a1ec-846535d5ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰åº“å·²æˆåŠŸå¯¼å…¥ï¼\n",
      "PyTorchç‰ˆæœ¬: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from Bio import SeqIO # è¿™æ˜¯Biopythonä¸­ç”¨äºè¯»å–FASTAæ–‡ä»¶çš„æ ¸å¿ƒæ¨¡å—\n",
    "\n",
    "print(\"æ‰€æœ‰åº“å·²æˆåŠŸå¯¼å…¥ï¼\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5470e43-d94e-4fe0-8f51-516352153d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®å¤„ç†\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# æ·±åº¦å­¦ä¹ \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ç”Ÿç‰©å­¦æ•°æ®å¤„ç†ï¼ˆæ ¸å¿ƒï¼ï¼‰\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "print(\"æ‰€æœ‰åº“å¯¼å…¥æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d828ebdf-9772-46bd-9a31-4a961678f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°†è¦å¤„ç†ä»¥ä¸‹æ–‡ä»¶ï¼š\n",
      "1. C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCF_000001405.40\\protein.faa\n",
      "2. C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCA_000001405.29\\protein.faa\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰ä¸¤ä¸ªFASTAæ–‡ä»¶çš„å®Œæ•´è·¯å¾„åˆ—è¡¨\n",
    "fasta_file_paths = [\n",
    "    r\"C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCF_000001405.40\\protein.faa\",  # ä¸»æ–‡ä»¶\n",
    "    r\"C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCA_000001405.29\\protein.faa\"   # è¡¥å……æ–‡ä»¶\n",
    "]\n",
    "\n",
    "print(\"å°†è¦å¤„ç†ä»¥ä¸‹æ–‡ä»¶ï¼š\")\n",
    "for i, path in enumerate(fasta_file_paths, 1):\n",
    "    print(f\"{i}. {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0efd6d-8c00-4304-bf01-db09a141afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta_file(file_path):\n",
    "    \"\"\"\n",
    "    è¯»å–ä¸€ä¸ªFASTAæ–‡ä»¶ï¼Œè¿”å›åŒ…å«è›‹ç™½è´¨ä¿¡æ¯çš„DataFrame\n",
    "    \"\"\"\n",
    "    protein_ids = []\n",
    "    protein_descriptions = []\n",
    "    protein_sequences = []\n",
    "    \n",
    "    print(f\"å¼€å§‹è¯»å–æ–‡ä»¶: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "            protein_ids.append(record.id)\n",
    "            protein_descriptions.append(record.description)\n",
    "            protein_sequences.append(str(record.seq))\n",
    "            \n",
    "        # åˆ›å»ºDataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'protein_id': protein_ids,\n",
    "            'description': protein_descriptions,\n",
    "            'sequence': protein_sequences,\n",
    "            'sequence_length': [len(seq) for seq in protein_sequences],\n",
    "            'source_file': file_path.split('\\\\')[-2]  # è®°å½•æ•°æ®æ¥æºï¼Œå¦‚ GCF_000001405.40\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ… æˆåŠŸè¯»å– {len(df)} æ¡åºåˆ—\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064910e3-c764-4aae-999e-e26834390b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¼€å§‹è¯»å–æ–‡ä»¶: C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCF_000001405.40\\protein.faa\n",
      "âœ… æˆåŠŸè¯»å– 136807 æ¡åºåˆ—\n",
      "--------------------------------------------------\n",
      "å¼€å§‹è¯»å–æ–‡ä»¶: C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCA_000001405.29\\protein.faa\n",
      "âœ… æˆåŠŸè¯»å– 13 æ¡åºåˆ—\n",
      "--------------------------------------------------\n",
      "åˆå¹¶åæ€»åºåˆ—æ•°: 136820\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨æ¯ä¸ªæ–‡ä»¶çš„æ•°æ®\n",
    "all_dfs = []\n",
    "\n",
    "# å¾ªç¯å¤„ç†æ¯ä¸ªFASTAæ–‡ä»¶\n",
    "for file_path in fasta_file_paths:\n",
    "    df = parse_fasta_file(file_path)\n",
    "    if df is not None:\n",
    "        all_dfs.append(df)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æˆåŠŸè¯»å–äº†æ–‡ä»¶\n",
    "if not all_dfs:\n",
    "    print(\"æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "else:\n",
    "    # å°†æ‰€æœ‰DataFrameåˆå¹¶æˆä¸€ä¸ª\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"åˆå¹¶åæ€»åºåˆ—æ•°: {len(combined_df)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6955f60a-6724-4662-9443-11a45f9a2c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå¹¶åçš„æ•°æ®é¢„è§ˆï¼š\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>description</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP_000005.3</td>\n",
       "      <td>NP_000005.3 alpha-2-macroglobulin isoform a pr...</td>\n",
       "      <td>MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...</td>\n",
       "      <td>1474</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP_000006.2</td>\n",
       "      <td>NP_000006.2 arylamine N-acetyltransferase 2 [H...</td>\n",
       "      <td>MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...</td>\n",
       "      <td>290</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP_000007.1</td>\n",
       "      <td>NP_000007.1 medium-chain specific acyl-CoA deh...</td>\n",
       "      <td>MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...</td>\n",
       "      <td>421</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NP_000008.1</td>\n",
       "      <td>NP_000008.1 short-chain specific acyl-CoA dehy...</td>\n",
       "      <td>MAAALLARASGPARRALCPRAWRQLHTIYQSVELPETHQMLLQTCR...</td>\n",
       "      <td>412</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NP_000009.1</td>\n",
       "      <td>NP_000009.1 very long-chain specific acyl-CoA ...</td>\n",
       "      <td>MQAARMAASLGRQLLRLGGGSSRLTALLGQPRPGPARRPYAGGAAQ...</td>\n",
       "      <td>655</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    protein_id                                        description  \\\n",
       "0  NP_000005.3  NP_000005.3 alpha-2-macroglobulin isoform a pr...   \n",
       "1  NP_000006.2  NP_000006.2 arylamine N-acetyltransferase 2 [H...   \n",
       "2  NP_000007.1  NP_000007.1 medium-chain specific acyl-CoA deh...   \n",
       "3  NP_000008.1  NP_000008.1 short-chain specific acyl-CoA dehy...   \n",
       "4  NP_000009.1  NP_000009.1 very long-chain specific acyl-CoA ...   \n",
       "\n",
       "                                            sequence  sequence_length  \\\n",
       "0  MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...             1474   \n",
       "1  MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...              290   \n",
       "2  MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...              421   \n",
       "3  MAAALLARASGPARRALCPRAWRQLHTIYQSVELPETHQMLLQTCR...              412   \n",
       "4  MQAARMAASLGRQLLRLGGGSSRLTALLGQPRPGPARRPYAGGAAQ...              655   \n",
       "\n",
       "        source_file  \n",
       "0  GCF_000001405.40  \n",
       "1  GCF_000001405.40  \n",
       "2  GCF_000001405.40  \n",
       "3  GCF_000001405.40  \n",
       "4  GCF_000001405.40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "åˆå¹¶åæ•°æ®å½¢çŠ¶: (136820, 5)\n",
      "é‡å¤çš„è›‹ç™½è´¨IDæ•°é‡: 0\n",
      "é‡å¤çš„è›‹ç™½è´¨åºåˆ—æ•°é‡: 46447\n",
      "å»é‡åçš„å”¯ä¸€åºåˆ—æ•°: 90373\n",
      "\n",
      "æ•°æ®æ¥æºåˆ†å¸ƒï¼š\n",
      "source_file\n",
      "GCF_000001405.40    90373\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥çœ‹åˆå¹¶åçš„æ•°æ®åŸºæœ¬ä¿¡æ¯\n",
    "print(\"åˆå¹¶åçš„æ•°æ®é¢„è§ˆï¼š\")\n",
    "display(combined_df.head())\n",
    "print(f\"\\nåˆå¹¶åæ•°æ®å½¢çŠ¶: {combined_df.shape}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„è›‹ç™½è´¨ID\n",
    "duplicate_ids = combined_df['protein_id'].duplicated().sum()\n",
    "print(f\"é‡å¤çš„è›‹ç™½è´¨IDæ•°é‡: {duplicate_ids}\")\n",
    "\n",
    "# åŸºäºè›‹ç™½è´¨åºåˆ—å»é‡ï¼ˆæ›´ä¸¥æ ¼ï¼Œå› ä¸ºä¸åŒIDå¯èƒ½æœ‰ç›¸åŒåºåˆ—ï¼‰\n",
    "duplicate_sequences = combined_df['sequence'].duplicated().sum()\n",
    "print(f\"é‡å¤çš„è›‹ç™½è´¨åºåˆ—æ•°é‡: {duplicate_sequences}\")\n",
    "\n",
    "# å»é™¤é‡å¤åºåˆ—ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„\n",
    "final_df = combined_df.drop_duplicates(subset=['sequence'], keep='first')\n",
    "print(f\"å»é‡åçš„å”¯ä¸€åºåˆ—æ•°: {len(final_df)}\")\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®æ¥æºåˆ†å¸ƒ\n",
    "print(\"\\næ•°æ®æ¥æºåˆ†å¸ƒï¼š\")\n",
    "print(final_df['source_file'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79733e93-061e-4148-8d87-1ed7e2864ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤„ç†å¥½çš„æ•°æ®å·²ä¿å­˜åˆ°: C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\n",
      "\n",
      "æœ€ç»ˆæ•°æ®é›†ä¿¡æ¯ï¼š\n",
      "æ€»è›‹ç™½è´¨åºåˆ—æ•°: 90373\n",
      "åºåˆ—é•¿åº¦èŒƒå›´: 12 åˆ° 35991 ä¸ªæ°¨åŸºé…¸\n",
      "å¹³å‡åºåˆ—é•¿åº¦: 715.7 ä¸ªæ°¨åŸºé…¸\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜å¤„ç†å¥½çš„æ•°æ®\n",
    "output_path = r\"C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\"\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"å¤„ç†å¥½çš„æ•°æ®å·²ä¿å­˜åˆ°: {output_path}\")\n",
    "\n",
    "# æŸ¥çœ‹æœ€ç»ˆæ•°æ®é›†çš„ä¿¡æ¯\n",
    "print(\"\\næœ€ç»ˆæ•°æ®é›†ä¿¡æ¯ï¼š\")\n",
    "print(f\"æ€»è›‹ç™½è´¨åºåˆ—æ•°: {len(final_df)}\")\n",
    "print(f\"åºåˆ—é•¿åº¦èŒƒå›´: {final_df['sequence_length'].min()} åˆ° {final_df['sequence_length'].max()} ä¸ªæ°¨åŸºé…¸\")\n",
    "print(f\"å¹³å‡åºåˆ—é•¿åº¦: {final_df['sequence_length'].mean():.1f} ä¸ªæ°¨åŸºé…¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b37a821-1422-46f9-9ee1-89c7b0659366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½Day 1å¤„ç†å¥½çš„æ•°æ®...\n",
      " æ•°æ®åŠ è½½æˆåŠŸï¼æ•°æ®é›†å½¢çŠ¶: (90373, 5)\n",
      "\n",
      " æ•°æ®é¢„è§ˆ:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>description</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP_000005.3</td>\n",
       "      <td>NP_000005.3 alpha-2-macroglobulin isoform a pr...</td>\n",
       "      <td>MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...</td>\n",
       "      <td>1474</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP_000006.2</td>\n",
       "      <td>NP_000006.2 arylamine N-acetyltransferase 2 [H...</td>\n",
       "      <td>MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...</td>\n",
       "      <td>290</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP_000007.1</td>\n",
       "      <td>NP_000007.1 medium-chain specific acyl-CoA deh...</td>\n",
       "      <td>MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...</td>\n",
       "      <td>421</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    protein_id                                        description  \\\n",
       "0  NP_000005.3  NP_000005.3 alpha-2-macroglobulin isoform a pr...   \n",
       "1  NP_000006.2  NP_000006.2 arylamine N-acetyltransferase 2 [H...   \n",
       "2  NP_000007.1  NP_000007.1 medium-chain specific acyl-CoA deh...   \n",
       "\n",
       "                                            sequence  sequence_length  \\\n",
       "0  MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...             1474   \n",
       "1  MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...              290   \n",
       "2  MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...              421   \n",
       "\n",
       "        source_file  \n",
       "0  GCF_000001405.40  \n",
       "1  GCF_000001405.40  \n",
       "2  GCF_000001405.40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Day 2 ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥åº“å’ŒåŠ è½½æ•°æ® ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤ºï¼ˆç¡®ä¿å›¾è¡¨èƒ½æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡ï¼‰\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# åŠ è½½Day 1å¤„ç†å¥½çš„æ•°æ®\n",
    "print(\"æ­£åœ¨åŠ è½½Day 1å¤„ç†å¥½çš„æ•°æ®...\")\n",
    "df = pd.read_csv(r\"C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\")\n",
    "print(f\" æ•°æ®åŠ è½½æˆåŠŸï¼æ•°æ®é›†å½¢çŠ¶: {df.shape}\")\n",
    "\n",
    "# æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®ï¼Œç¡®è®¤åŠ è½½æ­£ç¡®\n",
    "print(\"\\n æ•°æ®é¢„è§ˆ:\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e83461-297b-4a8e-b541-05b95feb1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åˆ›å»ºé…¶/éé…¶åˆ†ç±»æ ‡ç­¾...\n",
      "\n",
      " é…¶/éé…¶åˆ†ç±»ç»Ÿè®¡ï¼š\n",
      "éé…¶ (0): 76370 æ¡åºåˆ—\n",
      "é…¶ (1): 14003 æ¡åºåˆ—\n",
      "é…¶çš„æ¯”ä¾‹: 15.49%\n",
      "  æ³¨æ„ï¼šç±»åˆ«å­˜åœ¨ä¸å¹³è¡¡ï¼Œå°†åœ¨è®­ç»ƒæ—¶ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 ç¬¬äºŒæ­¥ï¼šåˆ›å»ºåˆ†ç±»æ ‡ç­¾ ---\n",
    "\n",
    "# å®šä¹‰é…¶ç›¸å…³çš„å…³é”®è¯\n",
    "enzyme_keywords = [\n",
    "    'kinase', 'polymerase', 'transferase', 'hydrolase', \n",
    "    'oxidoreductase', 'lyase', 'isomerase', 'ligase', \n",
    "    'reductase', 'synthase', 'enzyme', 'dehydrogenase'\n",
    "]\n",
    "\n",
    "def is_enzyme(description):\n",
    "    \"\"\"æ ¹æ®è›‹ç™½è´¨æè¿°åˆ¤æ–­æ˜¯å¦ä¸ºé…¶\"\"\"\n",
    "    if pd.isna(description):\n",
    "        return False\n",
    "    description_lower = str(description).lower()\n",
    "    return any(keyword in description_lower for keyword in enzyme_keywords)\n",
    "\n",
    "# æ·»åŠ é…¶/éé…¶æ ‡ç­¾åˆ—\n",
    "print(\"æ­£åœ¨åˆ›å»ºé…¶/éé…¶åˆ†ç±»æ ‡ç­¾...\")\n",
    "df['is_enzyme'] = df['description'].apply(is_enzyme).astype(int)\n",
    "\n",
    "# æŸ¥çœ‹æ ‡ç­¾åˆ†å¸ƒ\n",
    "label_counts = df['is_enzyme'].value_counts()\n",
    "print(\"\\n é…¶/éé…¶åˆ†ç±»ç»Ÿè®¡ï¼š\")\n",
    "print(f\"éé…¶ (0): {label_counts[0]} æ¡åºåˆ—\")\n",
    "print(f\"é…¶ (1): {label_counts[1]} æ¡åºåˆ—\")\n",
    "print(f\"é…¶çš„æ¯”ä¾‹: {df['is_enzyme'].mean():.2%}\")\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§\n",
    "if label_counts.min() / label_counts.sum() < 0.3:\n",
    "    print(\"  æ³¨æ„ï¼šç±»åˆ«å­˜åœ¨ä¸å¹³è¡¡ï¼Œå°†åœ¨è®­ç»ƒæ—¶ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°\")\n",
    "else:\n",
    "    print(\" ç±»åˆ«åˆ†å¸ƒç›¸å¯¹å¹³è¡¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a590706-e023-4656-931e-930e208dd59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ­£åœ¨åˆ›å»ºæ°¨åŸºé…¸è¯æ±‡è¡¨...\n",
      " è¯æ±‡è¡¨åˆ›å»ºå®Œæˆï¼å…± 24 ä¸ªå­—ç¬¦\n",
      "ç¤ºä¾‹æ˜ å°„: [('A', 1), ('C', 2), ('D', 3), ('E', 4), ('F', 5), ('G', 6), ('H', 7), ('I', 8)]\n",
      "\n",
      "è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦: 1500\n",
      "æ­£åœ¨è½¬æ¢åºåˆ—ä¸ºæ•°å­—ID...\n",
      " åºåˆ—æ ‡è®°åŒ–å®Œæˆï¼\n",
      "æˆªæ–­åçš„åºåˆ—é•¿åº¦ç»Ÿè®¡:\n",
      "count    90373.000000\n",
      "mean       636.772144\n",
      "std        412.675862\n",
      "min         12.000000\n",
      "25%        315.000000\n",
      "50%        520.000000\n",
      "75%        870.000000\n",
      "max       1500.000000\n",
      "Name: seq_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºè¯æ±‡è¡¨å’Œæ ‡è®°åŒ– ---\n",
    "\n",
    "print(\"\\næ­£åœ¨åˆ›å»ºæ°¨åŸºé…¸è¯æ±‡è¡¨...\")\n",
    "\n",
    "# è·å–æ‰€æœ‰å”¯ä¸€çš„æ°¨åŸºé…¸å­—ç¬¦\n",
    "all_sequences = ''.join(df['sequence'].tolist())\n",
    "unique_aa = sorted(set(all_sequences))\n",
    "\n",
    "# åˆ›å»ºæ°¨åŸºé…¸åˆ°IDçš„æ˜ å°„å­—å…¸\n",
    "vocab = {aa: idx+1 for idx, aa in enumerate(unique_aa)}  # ä»1å¼€å§‹ç¼–å·\n",
    "vocab['<PAD>'] = 0    # å¡«å……æ ‡è®°\n",
    "vocab['<UNK>'] = len(vocab)  # æœªçŸ¥æ°¨åŸºé…¸æ ‡è®°\n",
    "\n",
    "print(f\" è¯æ±‡è¡¨åˆ›å»ºå®Œæˆï¼å…± {len(vocab)} ä¸ªå­—ç¬¦\")\n",
    "print(\"ç¤ºä¾‹æ˜ å°„:\", list(vocab.items())[:8])\n",
    "\n",
    "# è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆåŸºäºæ‚¨ä¹‹å‰çš„æ•°æ®ï¼šå¹³å‡715ï¼Œæœ€å¤§35991ï¼Œæˆ‘ä»¬å–ä¸€ä¸ªåˆç†çš„å€¼ï¼‰\n",
    "MAX_SEQ_LENGTH = 1500  # è¿™ä¸ªå€¼å¯ä»¥è¦†ç›–å¤§éƒ¨åˆ†åºåˆ—\n",
    "print(f\"\\nè®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "# å°†æ°¨åŸºé…¸åºåˆ—è½¬æ¢ä¸ºæ•°å­—IDåºåˆ—\n",
    "def sequence_to_ids(sequence, vocab, max_len=None):\n",
    "    \"\"\"å°†è›‹ç™½è´¨åºåˆ—è½¬æ¢ä¸ºæ•°å­—IDåºåˆ—\"\"\"\n",
    "    ids = [vocab.get(aa, vocab['<UNK>']) for aa in sequence]\n",
    "    if max_len:\n",
    "        ids = ids[:max_len]  # æˆªæ–­é•¿åºåˆ—\n",
    "    return ids\n",
    "\n",
    "print(\"æ­£åœ¨è½¬æ¢åºåˆ—ä¸ºæ•°å­—ID...\")\n",
    "df['token_ids'] = df['sequence'].apply(lambda x: sequence_to_ids(x, vocab, MAX_SEQ_LENGTH))\n",
    "df['seq_len'] = df['token_ids'].apply(len)\n",
    "\n",
    "print(\" åºåˆ—æ ‡è®°åŒ–å®Œæˆï¼\")\n",
    "print(f\"æˆªæ–­åçš„åºåˆ—é•¿åº¦ç»Ÿè®¡:\")\n",
    "print(df['seq_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f6fc13-7c4f-4c0a-833f-b3b918568ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ­£åœ¨åˆ’åˆ†æ•°æ®é›†å¹¶åˆ›å»ºæ•°æ®åŠ è½½å™¨...\n",
      "æ•°æ®åŠ è½½æˆåŠŸï¼å½¢çŠ¶: (90373, 5)\n",
      "è®­ç»ƒé›†: 63261 æ¡åºåˆ—\n",
      "éªŒè¯é›†: 13556 æ¡åºåˆ—\n",
      "æµ‹è¯•é›†: 13556 æ¡åºåˆ—\n",
      " æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼æ‰¹æ¬¡å¤§å°: 4 (å·²è°ƒæ•´ä¸ºè¾ƒå°å€¼ä»¥é¿å…å†…å­˜ä¸è¶³)\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 æ­¥éª¤4: åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨ ---\n",
    "# é¦–å…ˆå¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"\\næ­£åœ¨åˆ’åˆ†æ•°æ®é›†å¹¶åˆ›å»ºæ•°æ®åŠ è½½å™¨...\")\n",
    "\n",
    "# é‡æ–°åŠ è½½æ•°æ®ï¼ˆç¡®ä¿dfå˜é‡å­˜åœ¨ï¼‰\n",
    "df = pd.read_csv(r\"C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\")\n",
    "print(f\"æ•°æ®åŠ è½½æˆåŠŸï¼å½¢çŠ¶: {df.shape}\")\n",
    "\n",
    "# é‡æ–°åˆ›å»ºé…¶æ ‡ç­¾ï¼ˆç¡®ä¿is_enzymeåˆ—å­˜åœ¨ï¼‰\n",
    "enzyme_keywords = [\n",
    "    'kinase', 'polymerase', 'transferase', 'hydrolase', 'oxidoreductase',\n",
    "    'lyase', 'isomerase', 'ligase', 'reductase', 'synthase', 'enzyme', 'dehydrogenase'\n",
    "]\n",
    "\n",
    "def is_enzyme(description):\n",
    "    if pd.isna(description):\n",
    "        return False\n",
    "    description_lower = str(description).lower()\n",
    "    return any(keyword in description_lower for keyword in enzyme_keywords)\n",
    "\n",
    "df['is_enzyme'] = df['description'].apply(is_enzyme).astype(int)\n",
    "\n",
    "# é‡æ–°è¿›è¡Œæ ‡è®°åŒ–ï¼ˆç¡®ä¿token_idsåˆ—å­˜åœ¨ï¼‰\n",
    "vocab = {}  # è¿™é‡Œéœ€è¦æ‚¨çš„è¯æ±‡è¡¨ï¼Œæˆ–è€…é‡æ–°åˆ›å»º\n",
    "# å¦‚æœæ‚¨æœ‰ä¿å­˜çš„è¯æ±‡è¡¨ï¼Œå¯ä»¥åŠ è½½ï¼š\n",
    "# vocab_df = pd.read_csv(r\"C:\\Users\\å¥¶ç½—å’ª\\Desktop\\ncbi_dataset\\amino_acid_vocab.csv\")\n",
    "# vocab = dict(zip(vocab_df['amino_acid'], vocab_df['token_id']))\n",
    "\n",
    "# æˆ–è€…é‡æ–°åˆ›å»ºè¯æ±‡è¡¨\n",
    "all_sequences = ''.join(df['sequence'].tolist())\n",
    "unique_aa = sorted(set(all_sequences))\n",
    "vocab = {aa: idx+1 for idx, aa in enumerate(unique_aa)}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = len(vocab)\n",
    "\n",
    "MAX_SEQ_LENGTH = 1500\n",
    "def sequence_to_ids(sequence, vocab, max_len=None):\n",
    "    ids = [vocab.get(aa, vocab['<UNK>']) for aa in sequence]\n",
    "    if max_len:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "df['token_ids'] = df['sequence'].apply(lambda x: sequence_to_ids(x, vocab, MAX_SEQ_LENGTH))\n",
    "\n",
    "# ç°åœ¨è¿›è¡Œæ•°æ®åˆ’åˆ†\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['is_enzyme'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['is_enzyme'])\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {len(train_df)} æ¡åºåˆ—\")\n",
    "print(f\"éªŒè¯é›†: {len(val_df)} æ¡åºåˆ—\")\n",
    "print(f\"æµ‹è¯•é›†: {len(test_df)} æ¡åºåˆ—\")\n",
    "\n",
    "# åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ç±»\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, max_len=MAX_SEQ_LENGTH):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # å¡«å……åºåˆ—åˆ°å›ºå®šé•¿åº¦\n",
    "        padded_seq = seq + [0] * (self.max_len - len(seq))\n",
    "        padded_seq = padded_seq[:self.max_len]\n",
    "        \n",
    "        return torch.tensor(padded_seq, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†å®ä¾‹\n",
    "train_dataset = ProteinDataset(train_df['token_ids'].tolist(), train_df['is_enzyme'].tolist())\n",
    "val_dataset = ProteinDataset(val_df['token_ids'].tolist(), val_df['is_enzyme'].tolist())\n",
    "test_dataset = ProteinDataset(test_df['token_ids'].tolist(), test_df['is_enzyme'].tolist())\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "BATCH_SIZE = 4  # ä» 32 æ”¹ä¸º 4ï¼Œè§£å†³å†…å­˜ä¸è¶³é—®é¢˜\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\" æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} (å·²è°ƒæ•´ä¸ºè¾ƒå°å€¼ä»¥é¿å…å†…å­˜ä¸è¶³)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8ef87e-a6ad-4656-b5ab-b08ff16d3198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ­£åœ¨æ„å»ºTransformeræ¨¡å‹...\n",
      "âœ… Transformeræ¨¡å‹æ„å»ºå®Œæˆï¼\n",
      "æ¨¡å‹å‚æ•°é‡: 804,546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\å¥¶ç½—å’ª\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 æ­¥éª¤5: æ„å»ºTransformeræ¨¡å‹ (ä¿®æ­£ç‰ˆ) ---\n",
    "# é¦–å…ˆå¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "print(\"\\næ­£åœ¨æ„å»ºTransformeræ¨¡å‹...\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class ProteinTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=4, num_classes=2):\n",
    "        super().__init__()\n",
    "        # æ·»åŠ è¿™ä¸€è¡Œï¼šä¿å­˜d_modelä¸ºç±»å±æ€§\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, MAX_SEQ_LENGTH)\n",
    "        \n",
    "        # Transformerç¼–ç å™¨\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=512, dropout=0.1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # åˆ†ç±»å™¨\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # srcå½¢çŠ¶: [batch_size, seq_len]\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)  # [batch_size, seq_len, d_model]\n",
    "        src = src.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)  # [seq_len, batch_size, d_model]\n",
    "        output = output.mean(dim=0)  # å…¨å±€å¹³å‡æ± åŒ– [batch_size, d_model]\n",
    "        return self.classifier(output)  # [batch_size, num_classes]\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹å®ä¾‹\n",
    "model = ProteinTransformer(vocab_size=len(vocab))\n",
    "print(\"âœ… Transformeræ¨¡å‹æ„å»ºå®Œæˆï¼\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6958283-d7ff-4d75-9bf3-88efb5781fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\å¥¶ç½—å’ª\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transformeræ¨¡å‹æ„å»ºå®Œæˆï¼\n",
      "æ¨¡å‹å‚æ•°é‡: 804,546\n",
      "ä½¿ç”¨è®¾å¤‡: cpu\n",
      "æ­£åœ¨è®¡ç®—ç±»åˆ«æƒé‡...\n",
      "æ ‡ç­¾åˆ†å¸ƒ: éé…¶=76370, é…¶=14003\n",
      "ä½¿ç”¨ç±»åˆ«æƒé‡: [1.       5.453831]\n",
      "\n",
      "å¼€å§‹è®­ç»ƒæ¨¡å‹...\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 æ­¥éª¤6: è®­ç»ƒæ¨¡å‹ ---\n",
    "\n",
    "# 1. é‡æ–°åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆä½¿ç”¨ä¿®æ­£åçš„ç±»ï¼‰\n",
    "model = ProteinTransformer(vocab_size=len(vocab))\n",
    "print(\"âœ… Transformeræ¨¡å‹æ„å»ºå®Œæˆï¼\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 2. è®¾ç½®è®¾å¤‡\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆè§£å†³label_countsæœªå®šä¹‰çš„é—®é¢˜ï¼‰\n",
    "# é¦–å…ˆéœ€è¦è®¡ç®—æ ‡ç­¾çš„åˆ†å¸ƒ\n",
    "print(\"æ­£åœ¨è®¡ç®—ç±»åˆ«æƒé‡...\")\n",
    "\n",
    "# æ–¹æ³•1ï¼šå¦‚æœæ‚¨æœ‰dfå˜é‡ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—\n",
    "try:\n",
    "    label_counts = df['is_enzyme'].value_counts()\n",
    "    print(f\"æ ‡ç­¾åˆ†å¸ƒ: éé…¶={label_counts[0]}, é…¶={label_counts[1]}\")\n",
    "except:\n",
    "    # æ–¹æ³•2ï¼šå¦‚æœæ²¡æœ‰dfï¼Œä»æ•°æ®åŠ è½½å™¨ä¸­ç»Ÿè®¡\n",
    "    enzyme_count = 0\n",
    "    non_enzyme_count = 0\n",
    "    for data, target in train_loader:\n",
    "        enzyme_count += (target == 1).sum().item()\n",
    "        non_enzyme_count += (target == 0).sum().item()\n",
    "    label_counts = {0: non_enzyme_count, 1: enzyme_count}\n",
    "    print(f\"ä»è®­ç»ƒé›†ç»Ÿè®¡çš„æ ‡ç­¾åˆ†å¸ƒ: éé…¶={non_enzyme_count}, é…¶={enzyme_count}\")\n",
    "\n",
    "# è®¾ç½®æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰\n",
    "class_weights = torch.tensor([1.0, label_counts[0]/label_counts[1]], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"ä½¿ç”¨ç±»åˆ«æƒé‡: {class_weights.cpu().numpy()}\")\n",
    "\n",
    "# 4. è®­ç»ƒå¾ªç¯\n",
    "num_epochs = 5  # å…ˆè®­ç»ƒ5ä¸ªepochçœ‹æ•ˆæœ\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "print(\"\\nå¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # è®­ç»ƒé˜¶æ®µ\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # éªŒè¯é˜¶æ®µ\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, '\n",
    "          f'Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "print(\" æ¨¡å‹è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2154a46-61e7-4fb8-b051-a028140abdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°è¯•ç´§æ€¥ä¿å­˜...\n",
      "vocab ç±»å‹: <class 'dict'>, å¤§å°: 24\n",
      "BATCH_SIZE: 4\n",
      "âœ… è¯æ±‡è¡¨å·²ä¿å­˜åˆ°æ¡Œé¢: C:\\Users\\å¥¶ç½—å’ª\\Desktop\\protein_vocab.pkl\n",
      "âœ… é…ç½®å·²ä¿å­˜åˆ°æ¡Œé¢: C:\\Users\\å¥¶ç½—å’ª\\Desktop\\training_config.pkl\n",
      "\n",
      "æ¡Œé¢æ–‡ä»¶åˆ—è¡¨:\n",
      "ğŸ“„ protein_vocab.pkl\n",
      "ğŸ“„ training_config.pkl\n",
      "ç´§æ€¥ä¿å­˜å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# --- ç´§æ€¥ä¿å­˜ ---\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print(\"å°è¯•ç´§æ€¥ä¿å­˜...\")\n",
    "\n",
    "# 1. é¦–å…ˆæ£€æŸ¥æœ€é‡è¦çš„å˜é‡æ˜¯å¦å­˜åœ¨\n",
    "try:\n",
    "    print(f\"vocab ç±»å‹: {type(vocab)}, å¤§å°: {len(vocab)}\")\n",
    "    vocab_exists = True\n",
    "except:\n",
    "    print(\"vocab ä¸å­˜åœ¨\")\n",
    "    vocab_exists = False\n",
    "\n",
    "try:\n",
    "    print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "    batch_size_exists = True  \n",
    "except:\n",
    "    print(\"BATCH_SIZE ä¸å­˜åœ¨\")\n",
    "    batch_size_exists = False\n",
    "\n",
    "# 2. å°è¯•ä¿å­˜åˆ°æ¡Œé¢ï¼ˆé¿å…æƒé™é—®é¢˜ï¼‰\n",
    "desktop_path = r\"C:\\Users\\å¥¶ç½—å’ª\\Desktop\"\n",
    "\n",
    "if vocab_exists:\n",
    "    try:\n",
    "        vocab_file = os.path.join(desktop_path, \"protein_vocab.pkl\")\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "        print(f\"âœ… è¯æ±‡è¡¨å·²ä¿å­˜åˆ°æ¡Œé¢: {vocab_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜å¤±è´¥: {e}\")\n",
    "\n",
    "# 3. ä¿å­˜é…ç½®\n",
    "config = {\n",
    "    'class_weights': [1.0, 5.453831],\n",
    "    'learning_rate': 0.001,\n",
    "    'BATCH_SIZE': BATCH_SIZE if batch_size_exists else 4,\n",
    "    'MAX_SEQ_LENGTH': 1500\n",
    "}\n",
    "\n",
    "try:\n",
    "    config_file = os.path.join(desktop_path, \"training_config.pkl\")\n",
    "    with open(config_file, 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "    print(f\"âœ… é…ç½®å·²ä¿å­˜åˆ°æ¡Œé¢: {config_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ é…ç½®ä¿å­˜å¤±è´¥: {e}\")\n",
    "\n",
    "# 4. æ£€æŸ¥æ¡Œé¢æ–‡ä»¶\n",
    "print(\"\\næ¡Œé¢æ–‡ä»¶åˆ—è¡¨:\")\n",
    "for file in os.listdir(desktop_path):\n",
    "    if file.endswith('.pkl'):\n",
    "        print(f\"ğŸ“„ {file}\")\n",
    "\n",
    "print(\"ç´§æ€¥ä¿å­˜å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e726e48-071d-44b4-a454-8ead23309f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcda26d-4276-44be-aab0-f0f733333733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a1f00-fa6f-421e-9249-2591c657e60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd7d0f-83f3-4986-996d-23ce07da38ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f96b24-0d4f-40cb-b2ad-b628094fe59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
