{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8156a045-db82-4fb1-99c0-d0c0bf379dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\奶罗咪\\appdata\\roaming\\python\\python313\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: biopython in c:\\users\\奶罗咪\\appdata\\roaming\\python\\python313\\site-packages (1.85)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\anaconda\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch pandas numpy scikit-learn biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135b3438-d4c2-48d4-a1ec-846535d5ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有库已成功导入！\n",
      "PyTorch版本: 2.8.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from Bio import SeqIO # 这是Biopython中用于读取FASTA文件的核心模块\n",
    "\n",
    "print(\"所有库已成功导入！\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5470e43-d94e-4fe0-8f51-516352153d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有库导入成功！\n"
     ]
    }
   ],
   "source": [
    "# 数据处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 深度学习\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 生物学数据处理（核心！）\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "print(\"所有库导入成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d828ebdf-9772-46bd-9a31-4a961678f6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将要处理以下文件：\n",
      "1. C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCF_000001405.40\\protein.faa\n",
      "2. C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCA_000001405.29\\protein.faa\n"
     ]
    }
   ],
   "source": [
    "# 定义两个FASTA文件的完整路径列表\n",
    "fasta_file_paths = [\n",
    "    r\"C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCF_000001405.40\\protein.faa\",  # 主文件\n",
    "    r\"C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCA_000001405.29\\protein.faa\"   # 补充文件\n",
    "]\n",
    "\n",
    "print(\"将要处理以下文件：\")\n",
    "for i, path in enumerate(fasta_file_paths, 1):\n",
    "    print(f\"{i}. {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0efd6d-8c00-4304-bf01-db09a141afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta_file(file_path):\n",
    "    \"\"\"\n",
    "    读取一个FASTA文件，返回包含蛋白质信息的DataFrame\n",
    "    \"\"\"\n",
    "    protein_ids = []\n",
    "    protein_descriptions = []\n",
    "    protein_sequences = []\n",
    "    \n",
    "    print(f\"开始读取文件: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "            protein_ids.append(record.id)\n",
    "            protein_descriptions.append(record.description)\n",
    "            protein_sequences.append(str(record.seq))\n",
    "            \n",
    "        # 创建DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'protein_id': protein_ids,\n",
    "            'description': protein_descriptions,\n",
    "            'sequence': protein_sequences,\n",
    "            'sequence_length': [len(seq) for seq in protein_sequences],\n",
    "            'source_file': file_path.split('\\\\')[-2]  # 记录数据来源，如 GCF_000001405.40\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ 成功读取 {len(df)} 条序列\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取文件时出错: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064910e3-c764-4aae-999e-e26834390b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读取文件: C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCF_000001405.40\\protein.faa\n",
      "✅ 成功读取 136807 条序列\n",
      "--------------------------------------------------\n",
      "开始读取文件: C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\ncbi_dataset\\data\\GCA_000001405.29\\protein.faa\n",
      "✅ 成功读取 13 条序列\n",
      "--------------------------------------------------\n",
      "合并后总序列数: 136820\n"
     ]
    }
   ],
   "source": [
    "# 创建一个空列表来存储每个文件的数据\n",
    "all_dfs = []\n",
    "\n",
    "# 循环处理每个FASTA文件\n",
    "for file_path in fasta_file_paths:\n",
    "    df = parse_fasta_file(file_path)\n",
    "    if df is not None:\n",
    "        all_dfs.append(df)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 检查是否成功读取了文件\n",
    "if not all_dfs:\n",
    "    print(\"没有成功读取任何文件，请检查路径是否正确。\")\n",
    "else:\n",
    "    # 将所有DataFrame合并成一个\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"合并后总序列数: {len(combined_df)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6955f60a-6724-4662-9443-11a45f9a2c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的数据预览：\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>description</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP_000005.3</td>\n",
       "      <td>NP_000005.3 alpha-2-macroglobulin isoform a pr...</td>\n",
       "      <td>MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...</td>\n",
       "      <td>1474</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP_000006.2</td>\n",
       "      <td>NP_000006.2 arylamine N-acetyltransferase 2 [H...</td>\n",
       "      <td>MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...</td>\n",
       "      <td>290</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP_000007.1</td>\n",
       "      <td>NP_000007.1 medium-chain specific acyl-CoA deh...</td>\n",
       "      <td>MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...</td>\n",
       "      <td>421</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NP_000008.1</td>\n",
       "      <td>NP_000008.1 short-chain specific acyl-CoA dehy...</td>\n",
       "      <td>MAAALLARASGPARRALCPRAWRQLHTIYQSVELPETHQMLLQTCR...</td>\n",
       "      <td>412</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NP_000009.1</td>\n",
       "      <td>NP_000009.1 very long-chain specific acyl-CoA ...</td>\n",
       "      <td>MQAARMAASLGRQLLRLGGGSSRLTALLGQPRPGPARRPYAGGAAQ...</td>\n",
       "      <td>655</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    protein_id                                        description  \\\n",
       "0  NP_000005.3  NP_000005.3 alpha-2-macroglobulin isoform a pr...   \n",
       "1  NP_000006.2  NP_000006.2 arylamine N-acetyltransferase 2 [H...   \n",
       "2  NP_000007.1  NP_000007.1 medium-chain specific acyl-CoA deh...   \n",
       "3  NP_000008.1  NP_000008.1 short-chain specific acyl-CoA dehy...   \n",
       "4  NP_000009.1  NP_000009.1 very long-chain specific acyl-CoA ...   \n",
       "\n",
       "                                            sequence  sequence_length  \\\n",
       "0  MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...             1474   \n",
       "1  MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...              290   \n",
       "2  MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...              421   \n",
       "3  MAAALLARASGPARRALCPRAWRQLHTIYQSVELPETHQMLLQTCR...              412   \n",
       "4  MQAARMAASLGRQLLRLGGGSSRLTALLGQPRPGPARRPYAGGAAQ...              655   \n",
       "\n",
       "        source_file  \n",
       "0  GCF_000001405.40  \n",
       "1  GCF_000001405.40  \n",
       "2  GCF_000001405.40  \n",
       "3  GCF_000001405.40  \n",
       "4  GCF_000001405.40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "合并后数据形状: (136820, 5)\n",
      "重复的蛋白质ID数量: 0\n",
      "重复的蛋白质序列数量: 46447\n",
      "去重后的唯一序列数: 90373\n",
      "\n",
      "数据来源分布：\n",
      "source_file\n",
      "GCF_000001405.40    90373\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 查看合并后的数据基本信息\n",
    "print(\"合并后的数据预览：\")\n",
    "display(combined_df.head())\n",
    "print(f\"\\n合并后数据形状: {combined_df.shape}\")\n",
    "\n",
    "# 检查是否有重复的蛋白质ID\n",
    "duplicate_ids = combined_df['protein_id'].duplicated().sum()\n",
    "print(f\"重复的蛋白质ID数量: {duplicate_ids}\")\n",
    "\n",
    "# 基于蛋白质序列去重（更严格，因为不同ID可能有相同序列）\n",
    "duplicate_sequences = combined_df['sequence'].duplicated().sum()\n",
    "print(f\"重复的蛋白质序列数量: {duplicate_sequences}\")\n",
    "\n",
    "# 去除重复序列，保留第一个出现的\n",
    "final_df = combined_df.drop_duplicates(subset=['sequence'], keep='first')\n",
    "print(f\"去重后的唯一序列数: {len(final_df)}\")\n",
    "\n",
    "# 查看数据来源分布\n",
    "print(\"\\n数据来源分布：\")\n",
    "print(final_df['source_file'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79733e93-061e-4148-8d87-1ed7e2864ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理好的数据已保存到: C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\n",
      "\n",
      "最终数据集信息：\n",
      "总蛋白质序列数: 90373\n",
      "序列长度范围: 12 到 35991 个氨基酸\n",
      "平均序列长度: 715.7 个氨基酸\n"
     ]
    }
   ],
   "source": [
    "# 保存处理好的数据\n",
    "output_path = r\"C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\"\n",
    "final_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"处理好的数据已保存到: {output_path}\")\n",
    "\n",
    "# 查看最终数据集的信息\n",
    "print(\"\\n最终数据集信息：\")\n",
    "print(f\"总蛋白质序列数: {len(final_df)}\")\n",
    "print(f\"序列长度范围: {final_df['sequence_length'].min()} 到 {final_df['sequence_length'].max()} 个氨基酸\")\n",
    "print(f\"平均序列长度: {final_df['sequence_length'].mean():.1f} 个氨基酸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b37a821-1422-46f9-9ee1-89c7b0659366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载Day 1处理好的数据...\n",
      " 数据加载成功！数据集形状: (90373, 5)\n",
      "\n",
      " 数据预览:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>description</th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_length</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP_000005.3</td>\n",
       "      <td>NP_000005.3 alpha-2-macroglobulin isoform a pr...</td>\n",
       "      <td>MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...</td>\n",
       "      <td>1474</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NP_000006.2</td>\n",
       "      <td>NP_000006.2 arylamine N-acetyltransferase 2 [H...</td>\n",
       "      <td>MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...</td>\n",
       "      <td>290</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NP_000007.1</td>\n",
       "      <td>NP_000007.1 medium-chain specific acyl-CoA deh...</td>\n",
       "      <td>MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...</td>\n",
       "      <td>421</td>\n",
       "      <td>GCF_000001405.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    protein_id                                        description  \\\n",
       "0  NP_000005.3  NP_000005.3 alpha-2-macroglobulin isoform a pr...   \n",
       "1  NP_000006.2  NP_000006.2 arylamine N-acetyltransferase 2 [H...   \n",
       "2  NP_000007.1  NP_000007.1 medium-chain specific acyl-CoA deh...   \n",
       "\n",
       "                                            sequence  sequence_length  \\\n",
       "0  MGKNKLLHPSLVLLLLVLLPTDASVSGKPQYMVLVPSLLHTETTEK...             1474   \n",
       "1  MDIEAYFERIGYKNSRNKLDLETLTDILEHQIRAVPFENLNMHCGQ...              290   \n",
       "2  MAAGFGRCCRVLRSISRFHWRSQHTKANRQREPGLGFSFEFTEQQK...              421   \n",
       "\n",
       "        source_file  \n",
       "0  GCF_000001405.40  \n",
       "1  GCF_000001405.40  \n",
       "2  GCF_000001405.40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Day 2 第一步：导入库和加载数据 ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文显示（确保图表能正常显示中文）\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 加载Day 1处理好的数据\n",
    "print(\"正在加载Day 1处理好的数据...\")\n",
    "df = pd.read_csv(r\"C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\")\n",
    "print(f\" 数据加载成功！数据集形状: {df.shape}\")\n",
    "\n",
    "# 显示前几行数据，确认加载正确\n",
    "print(\"\\n 数据预览:\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e83461-297b-4a8e-b541-05b95feb1a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在创建酶/非酶分类标签...\n",
      "\n",
      " 酶/非酶分类统计：\n",
      "非酶 (0): 76370 条序列\n",
      "酶 (1): 14003 条序列\n",
      "酶的比例: 15.49%\n",
      "  注意：类别存在不平衡，将在训练时使用加权损失函数\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 第二步：创建分类标签 ---\n",
    "\n",
    "# 定义酶相关的关键词\n",
    "enzyme_keywords = [\n",
    "    'kinase', 'polymerase', 'transferase', 'hydrolase', \n",
    "    'oxidoreductase', 'lyase', 'isomerase', 'ligase', \n",
    "    'reductase', 'synthase', 'enzyme', 'dehydrogenase'\n",
    "]\n",
    "\n",
    "def is_enzyme(description):\n",
    "    \"\"\"根据蛋白质描述判断是否为酶\"\"\"\n",
    "    if pd.isna(description):\n",
    "        return False\n",
    "    description_lower = str(description).lower()\n",
    "    return any(keyword in description_lower for keyword in enzyme_keywords)\n",
    "\n",
    "# 添加酶/非酶标签列\n",
    "print(\"正在创建酶/非酶分类标签...\")\n",
    "df['is_enzyme'] = df['description'].apply(is_enzyme).astype(int)\n",
    "\n",
    "# 查看标签分布\n",
    "label_counts = df['is_enzyme'].value_counts()\n",
    "print(\"\\n 酶/非酶分类统计：\")\n",
    "print(f\"非酶 (0): {label_counts[0]} 条序列\")\n",
    "print(f\"酶 (1): {label_counts[1]} 条序列\")\n",
    "print(f\"酶的比例: {df['is_enzyme'].mean():.2%}\")\n",
    "\n",
    "# 检查数据平衡性\n",
    "if label_counts.min() / label_counts.sum() < 0.3:\n",
    "    print(\"  注意：类别存在不平衡，将在训练时使用加权损失函数\")\n",
    "else:\n",
    "    print(\" 类别分布相对平衡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a590706-e023-4656-931e-930e208dd59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在创建氨基酸词汇表...\n",
      " 词汇表创建完成！共 24 个字符\n",
      "示例映射: [('A', 1), ('C', 2), ('D', 3), ('E', 4), ('F', 5), ('G', 6), ('H', 7), ('I', 8)]\n",
      "\n",
      "设置最大序列长度: 1500\n",
      "正在转换序列为数字ID...\n",
      " 序列标记化完成！\n",
      "截断后的序列长度统计:\n",
      "count    90373.000000\n",
      "mean       636.772144\n",
      "std        412.675862\n",
      "min         12.000000\n",
      "25%        315.000000\n",
      "50%        520.000000\n",
      "75%        870.000000\n",
      "max       1500.000000\n",
      "Name: seq_len, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 第三步：创建词汇表和标记化 ---\n",
    "\n",
    "print(\"\\n正在创建氨基酸词汇表...\")\n",
    "\n",
    "# 获取所有唯一的氨基酸字符\n",
    "all_sequences = ''.join(df['sequence'].tolist())\n",
    "unique_aa = sorted(set(all_sequences))\n",
    "\n",
    "# 创建氨基酸到ID的映射字典\n",
    "vocab = {aa: idx+1 for idx, aa in enumerate(unique_aa)}  # 从1开始编号\n",
    "vocab['<PAD>'] = 0    # 填充标记\n",
    "vocab['<UNK>'] = len(vocab)  # 未知氨基酸标记\n",
    "\n",
    "print(f\" 词汇表创建完成！共 {len(vocab)} 个字符\")\n",
    "print(\"示例映射:\", list(vocab.items())[:8])\n",
    "\n",
    "# 设置最大序列长度（基于您之前的数据：平均715，最大35991，我们取一个合理的值）\n",
    "MAX_SEQ_LENGTH = 1500  # 这个值可以覆盖大部分序列\n",
    "print(f\"\\n设置最大序列长度: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "# 将氨基酸序列转换为数字ID序列\n",
    "def sequence_to_ids(sequence, vocab, max_len=None):\n",
    "    \"\"\"将蛋白质序列转换为数字ID序列\"\"\"\n",
    "    ids = [vocab.get(aa, vocab['<UNK>']) for aa in sequence]\n",
    "    if max_len:\n",
    "        ids = ids[:max_len]  # 截断长序列\n",
    "    return ids\n",
    "\n",
    "print(\"正在转换序列为数字ID...\")\n",
    "df['token_ids'] = df['sequence'].apply(lambda x: sequence_to_ids(x, vocab, MAX_SEQ_LENGTH))\n",
    "df['seq_len'] = df['token_ids'].apply(len)\n",
    "\n",
    "print(\" 序列标记化完成！\")\n",
    "print(f\"截断后的序列长度统计:\")\n",
    "print(df['seq_len'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f6fc13-7c4f-4c0a-833f-b3b918568ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在划分数据集并创建数据加载器...\n",
      "数据加载成功！形状: (90373, 5)\n",
      "训练集: 63261 条序列\n",
      "验证集: 13556 条序列\n",
      "测试集: 13556 条序列\n",
      " 数据加载器创建完成！批次大小: 4 (已调整为较小值以避免内存不足)\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 步骤4: 创建PyTorch数据加载器 ---\n",
    "# 首先导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"\\n正在划分数据集并创建数据加载器...\")\n",
    "\n",
    "# 重新加载数据（确保df变量存在）\n",
    "df = pd.read_csv(r\"C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\processed_protein_data_combined.csv\")\n",
    "print(f\"数据加载成功！形状: {df.shape}\")\n",
    "\n",
    "# 重新创建酶标签（确保is_enzyme列存在）\n",
    "enzyme_keywords = [\n",
    "    'kinase', 'polymerase', 'transferase', 'hydrolase', 'oxidoreductase',\n",
    "    'lyase', 'isomerase', 'ligase', 'reductase', 'synthase', 'enzyme', 'dehydrogenase'\n",
    "]\n",
    "\n",
    "def is_enzyme(description):\n",
    "    if pd.isna(description):\n",
    "        return False\n",
    "    description_lower = str(description).lower()\n",
    "    return any(keyword in description_lower for keyword in enzyme_keywords)\n",
    "\n",
    "df['is_enzyme'] = df['description'].apply(is_enzyme).astype(int)\n",
    "\n",
    "# 重新进行标记化（确保token_ids列存在）\n",
    "vocab = {}  # 这里需要您的词汇表，或者重新创建\n",
    "# 如果您有保存的词汇表，可以加载：\n",
    "# vocab_df = pd.read_csv(r\"C:\\Users\\奶罗咪\\Desktop\\ncbi_dataset\\amino_acid_vocab.csv\")\n",
    "# vocab = dict(zip(vocab_df['amino_acid'], vocab_df['token_id']))\n",
    "\n",
    "# 或者重新创建词汇表\n",
    "all_sequences = ''.join(df['sequence'].tolist())\n",
    "unique_aa = sorted(set(all_sequences))\n",
    "vocab = {aa: idx+1 for idx, aa in enumerate(unique_aa)}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = len(vocab)\n",
    "\n",
    "MAX_SEQ_LENGTH = 1500\n",
    "def sequence_to_ids(sequence, vocab, max_len=None):\n",
    "    ids = [vocab.get(aa, vocab['<UNK>']) for aa in sequence]\n",
    "    if max_len:\n",
    "        ids = ids[:max_len]\n",
    "    return ids\n",
    "\n",
    "df['token_ids'] = df['sequence'].apply(lambda x: sequence_to_ids(x, vocab, MAX_SEQ_LENGTH))\n",
    "\n",
    "# 现在进行数据划分\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['is_enzyme'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['is_enzyme'])\n",
    "\n",
    "print(f\"训练集: {len(train_df)} 条序列\")\n",
    "print(f\"验证集: {len(val_df)} 条序列\")\n",
    "print(f\"测试集: {len(test_df)} 条序列\")\n",
    "\n",
    "# 创建自定义数据集类\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, max_len=MAX_SEQ_LENGTH):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 填充序列到固定长度\n",
    "        padded_seq = seq + [0] * (self.max_len - len(seq))\n",
    "        padded_seq = padded_seq[:self.max_len]\n",
    "        \n",
    "        return torch.tensor(padded_seq, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = ProteinDataset(train_df['token_ids'].tolist(), train_df['is_enzyme'].tolist())\n",
    "val_dataset = ProteinDataset(val_df['token_ids'].tolist(), val_df['is_enzyme'].tolist())\n",
    "test_dataset = ProteinDataset(test_df['token_ids'].tolist(), test_df['is_enzyme'].tolist())\n",
    "\n",
    "# 创建数据加载器\n",
    "BATCH_SIZE = 4  # 从 32 改为 4，解决内存不足问题\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\" 数据加载器创建完成！批次大小: {BATCH_SIZE} (已调整为较小值以避免内存不足)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8ef87e-a6ad-4656-b5ab-b08ff16d3198",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在构建Transformer模型...\n",
      "✅ Transformer模型构建完成！\n",
      "模型参数量: 804,546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\奶罗咪\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 步骤5: 构建Transformer模型 (修正版) ---\n",
    "# 首先导入所有必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "print(\"\\n正在构建Transformer模型...\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class ProteinTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=4, num_classes=2):\n",
    "        super().__init__()\n",
    "        # 添加这一行：保存d_model为类属性\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, MAX_SEQ_LENGTH)\n",
    "        \n",
    "        # Transformer编码器\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=512, dropout=0.1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src形状: [batch_size, seq_len]\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)  # [batch_size, seq_len, d_model]\n",
    "        src = src.transpose(0, 1)  # [seq_len, batch_size, d_model]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)  # [seq_len, batch_size, d_model]\n",
    "        output = output.mean(dim=0)  # 全局平均池化 [batch_size, d_model]\n",
    "        return self.classifier(output)  # [batch_size, num_classes]\n",
    "\n",
    "# 创建模型实例\n",
    "model = ProteinTransformer(vocab_size=len(vocab))\n",
    "print(\"✅ Transformer模型构建完成！\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6958283-d7ff-4d75-9bf3-88efb5781fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\奶罗咪\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformer模型构建完成！\n",
      "模型参数量: 804,546\n",
      "使用设备: cpu\n",
      "正在计算类别权重...\n",
      "标签分布: 非酶=76370, 酶=14003\n",
      "使用类别权重: [1.       5.453831]\n",
      "\n",
      "开始训练模型...\n"
     ]
    }
   ],
   "source": [
    "# --- Day 2 步骤6: 训练模型 ---\n",
    "\n",
    "# 1. 重新创建模型实例（使用修正后的类）\n",
    "model = ProteinTransformer(vocab_size=len(vocab))\n",
    "print(\"✅ Transformer模型构建完成！\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 2. 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. 计算类别权重（解决label_counts未定义的问题）\n",
    "# 首先需要计算标签的分布\n",
    "print(\"正在计算类别权重...\")\n",
    "\n",
    "# 方法1：如果您有df变量，可以直接计算\n",
    "try:\n",
    "    label_counts = df['is_enzyme'].value_counts()\n",
    "    print(f\"标签分布: 非酶={label_counts[0]}, 酶={label_counts[1]}\")\n",
    "except:\n",
    "    # 方法2：如果没有df，从数据加载器中统计\n",
    "    enzyme_count = 0\n",
    "    non_enzyme_count = 0\n",
    "    for data, target in train_loader:\n",
    "        enzyme_count += (target == 1).sum().item()\n",
    "        non_enzyme_count += (target == 0).sum().item()\n",
    "    label_counts = {0: non_enzyme_count, 1: enzyme_count}\n",
    "    print(f\"从训练集统计的标签分布: 非酶={non_enzyme_count}, 酶={enzyme_count}\")\n",
    "\n",
    "# 设置损失函数和优化器（处理类别不平衡）\n",
    "class_weights = torch.tensor([1.0, label_counts[0]/label_counts[1]], dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"使用类别权重: {class_weights.cpu().numpy()}\")\n",
    "\n",
    "# 4. 训练循环\n",
    "num_epochs = 5  # 先训练5个epoch看效果\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "print(\"\\n开始训练模型...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, '\n",
    "          f'Val Loss: {avg_val_loss:.4f}, '\n",
    "          f'Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "print(\" 模型训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2154a46-61e7-4fb8-b051-a028140abdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尝试紧急保存...\n",
      "vocab 类型: <class 'dict'>, 大小: 24\n",
      "BATCH_SIZE: 4\n",
      "✅ 词汇表已保存到桌面: C:\\Users\\奶罗咪\\Desktop\\protein_vocab.pkl\n",
      "✅ 配置已保存到桌面: C:\\Users\\奶罗咪\\Desktop\\training_config.pkl\n",
      "\n",
      "桌面文件列表:\n",
      "📄 protein_vocab.pkl\n",
      "📄 training_config.pkl\n",
      "紧急保存完成！\n"
     ]
    }
   ],
   "source": [
    "# --- 紧急保存 ---\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "print(\"尝试紧急保存...\")\n",
    "\n",
    "# 1. 首先检查最重要的变量是否存在\n",
    "try:\n",
    "    print(f\"vocab 类型: {type(vocab)}, 大小: {len(vocab)}\")\n",
    "    vocab_exists = True\n",
    "except:\n",
    "    print(\"vocab 不存在\")\n",
    "    vocab_exists = False\n",
    "\n",
    "try:\n",
    "    print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "    batch_size_exists = True  \n",
    "except:\n",
    "    print(\"BATCH_SIZE 不存在\")\n",
    "    batch_size_exists = False\n",
    "\n",
    "# 2. 尝试保存到桌面（避免权限问题）\n",
    "desktop_path = r\"C:\\Users\\奶罗咪\\Desktop\"\n",
    "\n",
    "if vocab_exists:\n",
    "    try:\n",
    "        vocab_file = os.path.join(desktop_path, \"protein_vocab.pkl\")\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            pickle.dump(vocab, f)\n",
    "        print(f\"✅ 词汇表已保存到桌面: {vocab_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存失败: {e}\")\n",
    "\n",
    "# 3. 保存配置\n",
    "config = {\n",
    "    'class_weights': [1.0, 5.453831],\n",
    "    'learning_rate': 0.001,\n",
    "    'BATCH_SIZE': BATCH_SIZE if batch_size_exists else 4,\n",
    "    'MAX_SEQ_LENGTH': 1500\n",
    "}\n",
    "\n",
    "try:\n",
    "    config_file = os.path.join(desktop_path, \"training_config.pkl\")\n",
    "    with open(config_file, 'wb') as f:\n",
    "        pickle.dump(config, f)\n",
    "    print(f\"✅ 配置已保存到桌面: {config_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 配置保存失败: {e}\")\n",
    "\n",
    "# 4. 检查桌面文件\n",
    "print(\"\\n桌面文件列表:\")\n",
    "for file in os.listdir(desktop_path):\n",
    "    if file.endswith('.pkl'):\n",
    "        print(f\"📄 {file}\")\n",
    "\n",
    "print(\"紧急保存完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e726e48-071d-44b4-a454-8ead23309f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcda26d-4276-44be-aab0-f0f733333733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77a1f00-fa6f-421e-9249-2591c657e60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd7d0f-83f3-4986-996d-23ce07da38ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f96b24-0d4f-40cb-b2ad-b628094fe59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
